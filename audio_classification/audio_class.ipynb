{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75622295",
   "metadata": {},
   "source": [
    "# Audio Classification System\n",
    "\n",
    "## Overview\n",
    "This notebook implements an automated audio classification system that:\n",
    "- Analyzes audio files with the YAMNet model\n",
    "- Performs AI-based categorization with OpenAI GPT-4\n",
    "- Exports results in XML and CSV formats\n",
    "- Generates JSON preset files for Max patch\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 PREREQUISITES & INSTALLATION\n",
    "\n",
    "### System Requirements\n",
    "- **Python 3.8 or newer** (Python 3.9-3.11 recommended)\n",
    "- **8GB RAM minimum** (16GB recommended for processing many files)\n",
    "- **Internet connection** (for downloading models and API access)\n",
    "\n",
    "### Step 1: Install Python\n",
    "If you don't have Python installed:\n",
    "\n",
    "**Windows:**\n",
    "1. Download from: https://www.python.org/downloads/\n",
    "2. During installation, check ✅ \"Add Python to PATH\"\n",
    "3. Complete the installation\n",
    "\n",
    "**macOS:**\n",
    "1. Python 3 is usually pre-installed\n",
    "2. Check version: Open Terminal and type `python3 --version`\n",
    "3. If not installed, download from: https://www.python.org/downloads/\n",
    "\n",
    "### Step 2: Install Required Libraries\n",
    "Open your **Terminal** (macOS) or **Command Prompt** (Windows) and run:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow tensorflow-hub librosa openai pandas\n",
    "```\n",
    "\n",
    "**If you encounter errors**, try:\n",
    "```bash\n",
    "pip3 install tensorflow tensorflow-hub librosa openai pandas\n",
    "```\n",
    "\n",
    "**Note:** Installation may take 5-10 minutes as TensorFlow is a large library.\n",
    "\n",
    "### Step 3: Get OpenAI API Key\n",
    "1. Go to: https://platform.openai.com/api-keys\n",
    "2. Sign up or log in to your OpenAI account\n",
    "3. Click \"Create new secret key\"\n",
    "4. Copy the key (you'll need it in Cell 2)\n",
    "\n",
    "**Important:** Keep your API key private! Never share it publicly.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 HOW TO USE THIS NOTEBOOK:\n",
    "\n",
    "### ✏️ STEP 1: Configure Settings (Cell 2 Below)\n",
    "- **YOU MUST EDIT** the configuration cell (Cell 2)\n",
    "- Set your folder path and API key\n",
    "- ⚠️ **CRITICAL**: Ensure audio files have NO umlauts, accents, or spaces!\n",
    "\n",
    "### ▶️ STEP 2: Run All Cells  \n",
    "- After configuration, run all cells in order\n",
    "- **DO NOT MODIFY** code in cells 3-5\n",
    "- Just click \"Run\" on each cell\n",
    "\n",
    "### ✅ STEP 3: Check Output\n",
    "- Classification XMLs will be saved with your audio files\n",
    "- JSON preset folders will be created in your material folder\n",
    "- See summary at the end for next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff0b9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✏️ CELL 1: CONFIGURATION - YOU MUST EDIT THIS!\n",
    "\n",
    "⚠️ **REQUIRED: Edit the values in the cell below**\n",
    "\n",
    "**What to change:**\n",
    "1. **folder_path**: Full path to your new audio material folder\n",
    "2. **api_key**: Your OpenAI API key (get from: https://platform.openai.com/api-keys)\n",
    "\n",
    "**File Naming Requirements (CRITICAL!):**\n",
    "- ❌ NO umlauts (ä, ö, ü)\n",
    "- ❌ NO accents (é, à, ñ)  \n",
    "- ❌ NO spaces (use underscores _)\n",
    "\n",
    "**Examples:**\n",
    "```\n",
    "❌ Wrong: \"Paco De Lucía.wav\"\n",
    "✅ Correct: \"Paco_De_Lucia.wav\"\n",
    "\n",
    "❌ Wrong: \"Müller & Söhne.wav\"\n",
    "✅ Correct: \"Mueller_und_Soehne.wav\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e651de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# ✏️ CONFIGURATION - EDIT THESE VALUES!\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ── Your Audio Material Folder Path ──────────────────────────────────────────\n",
    "# Replace with the FULL path to your new audio material folder\n",
    "# \n",
    "# Example Windows: \n",
    "#   folder_path = \"C:/Users/YourName/Documents/APO_Main/apo_material/new_sounds\"\n",
    "# \n",
    "# Example macOS: \n",
    "#   folder_path = \"/Users/YourName/Documents/APO_Main/apo_material/new_sounds\"\n",
    "\n",
    "folder_path = \"PUT_YOUR_FOLDER_PATH_HERE\"  # ← CHANGE THIS!\n",
    "\n",
    "# ── Your OpenAI API Key ──────────────────────────────────────────────────────\n",
    "# Replace with your OpenAI API key\n",
    "# Get your key from: https://platform.openai.com/api-keys\n",
    "\n",
    "api_key = \"PUT_YOUR_API_KEY_HERE\"  # ← CHANGE THIS!\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# ⚠️ DO NOT EDIT BELOW THIS LINE!\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Internal configuration (automatically set)\n",
    "csv_path = \"classes.csv\"  # Category definitions (must be in same folder as this notebook)\n",
    "xml_output_dir = folder_path  # XMLs will be saved in the material folder\n",
    "\n",
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"   Folder: {folder_path}\")\n",
    "print(f\"   Categories file: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917b4eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ▶️ CELLS 2-4: RUN THESE CELLS - DO NOT MODIFY!\n",
    "\n",
    "**Instructions:**\n",
    "1. Run Cell 2 (Load Dependencies)\n",
    "2. Run Cell 3 (Initialize Model & Analyze)\n",
    "3. Run Cell 4 (Generate JSON Presets)\n",
    "4. See summary at the end\n",
    "\n",
    "⚠️ **Do NOT modify** any code in the cells below!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# CELL 2: LOAD DEPENDENCIES\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# ▶️ RUN THIS CELL - DO NOT MODIFY!\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TROUBLESHOOTING: If you get \"ModuleNotFoundError\", the libraries are not \n",
    "# installed. Run this command in your Terminal/Command Prompt:\n",
    "#\n",
    "#   pip install tensorflow tensorflow-hub librosa openai pandas\n",
    "#\n",
    "# Then restart this notebook and try again.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"Loading libraries...\")\n",
    "\n",
    "import os, datetime, re, json, sys\n",
    "from pathlib import Path\n",
    "from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "from xml.dom import minidom\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Verify versions\n",
    "print(\"\\n📦 Library versions:\")\n",
    "print(f\"   TensorFlow: {tf.__version__}\")\n",
    "print(f\"   Librosa: {librosa.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\\n✅ All dependencies loaded successfully!\")\n",
    "print(\"   You can proceed to Cell 3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da22a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# CELL 3: INITIALIZE MODEL & ANALYZE AUDIO FILES\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# ▶️ RUN THIS CELL - DO NOT MODIFY!\n",
    "\n",
    "# Load YAMNet model\n",
    "print(\"Loading YAMNet model...\")\n",
    "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "class_map_path = tf.keras.utils.get_file('yamnet_class_map.csv',\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/yamnet/yamnet_class_map.csv')\n",
    "class_names = [line.split(',')[2] for line in open(class_map_path, encoding=\"utf-8\").readlines()[1:]]\n",
    "print(f\"✅ YAMNet loaded ({len(class_names)} classes)\\n\")\n",
    "\n",
    "# Load categories\n",
    "print(f\"Loading categories from: {csv_path}\")\n",
    "category_df = pd.read_csv(csv_path).dropna(subset=[\"Label\", \"Meaning Sound\"])\n",
    "categories = category_df[\"Label\"].tolist()\n",
    "descriptions = category_df[\"Meaning Sound\"].tolist()\n",
    "instruction_block = \"\\n\".join([f\"{cat}: {desc}\" for cat, desc in zip(categories, descriptions)])\n",
    "print(f\"✅ Loaded {len(categories)} categories\\n\")\n",
    "\n",
    "# GPT-4 prompt\n",
    "system_message = f\"\"\"You are an expert in perceptual sound classification.\n",
    "You will receive audio analysis data and return, for EVERY category below,\n",
    "(1) a value between 0 and 1 (how strongly the sound fits),\n",
    "(2) a confidence between 0 and 1 (your confidence in that value),\n",
    "(3) a single-sentence reasoning.\n",
    "\n",
    "Here are the category definitions:\n",
    "{instruction_block}\n",
    "\n",
    "STRICT OUTPUT FORMAT (one line per category; no extra text):\n",
    "Category | value | confidence | reasoning\"\"\"\n",
    "\n",
    "# Find audio files\n",
    "print(f\"Searching for audio files in: {folder_path}\")\n",
    "audio_files = []\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\")):\n",
    "            full_path = os.path.join(root, file)\n",
    "            audio_files.append((full_path, os.path.relpath(full_path, folder_path)))\n",
    "print(f\"✅ Found {len(audio_files)} audio files\\n\")\n",
    "\n",
    "# Utility functions\n",
    "def normalize(text):\n",
    "    return re.sub(r'\\W+', '', str(text)).lower()\n",
    "\n",
    "normalized_categories = {normalize(cat): cat for cat in categories}\n",
    "line_regexes = [\n",
    "    re.compile(r\"^\\s*(.+?)\\s*\\|\\s*([01](?:\\.\\d+)?)\\s*\\|\\s*([01](?:\\.\\d+)?)\\s*\\|\\s*(.+?)\\s*$\"),\n",
    "    re.compile(r\"^\\s*(.+?)\\s*:\\s*([01](?:\\.\\d+)?)\\s*\\|\\s*([01](?:\\.\\d+)?)\\s*\\|\\s*(.+?)\\s*$\"),\n",
    "]\n",
    "\n",
    "def parse_llm_lines(raw):\n",
    "    out = {}\n",
    "    for line in raw.splitlines():\n",
    "        if not line.strip(): continue\n",
    "        for rgx in line_regexes:\n",
    "            m = rgx.match(line)\n",
    "            if m:\n",
    "                key_raw, val_str, conf_str, reason = m.groups()\n",
    "                key_norm = normalize(key_raw)\n",
    "                if key_norm in normalized_categories:\n",
    "                    cat = normalized_categories[key_norm]\n",
    "                    try:\n",
    "                        val = max(0.0, min(1.0, float(val_str)))\n",
    "                        conf = max(0.0, min(1.0, float(conf_str)))\n",
    "                        out[cat] = (val, conf, reason.strip())\n",
    "                    except: pass\n",
    "                break\n",
    "    for cat in categories:\n",
    "        if cat not in out:\n",
    "            out[cat] = (0.0, 0.0, \"Not provided by model.\")\n",
    "    return out\n",
    "\n",
    "def pretty_xml(elem):\n",
    "    rough = tostring(elem, encoding=\"utf-8\")\n",
    "    return minidom.parseString(rough).toprettyxml(indent=\"  \", encoding=\"utf-8\")\n",
    "\n",
    "def analyze_audio(file_path):\n",
    "    waveform, sr = librosa.load(file_path, sr=16000)\n",
    "    scores, _, _ = yamnet_model(waveform)\n",
    "    mean_scores = tf.reduce_mean(scores, axis=0).numpy()\n",
    "    top_indices = mean_scores.argsort()[-15:][::-1]\n",
    "    top_labels = [(class_names[i], float(mean_scores[i])) for i in top_indices]\n",
    "    top_3_labels = \", \".join([label for label, _ in top_labels[:3]])\n",
    "    \n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(waveform)), ref=np.max)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sr)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sr)\n",
    "    dominant_freq = np.argmax(np.mean(np.abs(D), axis=1)) * (sr / 2 / D.shape[0])\n",
    "    rms = float(librosa.feature.rms(y=waveform).mean())\n",
    "    label_str = \", \".join([f\"{l} ({s:.2f})\" for l, s in top_labels])\n",
    "    \n",
    "    user_message = f\"\"\"Audio analysis:\n",
    "YAMNet top labels: {label_str}\n",
    "Dominant frequency: {dominant_freq:.2f} Hz\n",
    "Spectral centroid: {float(np.mean(spectral_centroid)):.2f}\n",
    "Spectral bandwidth: {float(np.mean(spectral_bandwidth)):.2f}\n",
    "Average loudness (RMS): {rms:.4f}\n",
    "\n",
    "Return one line PER CATEGORY exactly as:\n",
    "Category | value | confidence | reasoning\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message},\n",
    "                  {\"role\": \"user\", \"content\": user_message}], temperature=0.2)\n",
    "    triples = parse_llm_lines(response.choices[0].message.content)\n",
    "    \n",
    "    desc_response = client.chat.completions.create(model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in acoustic sound description.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"Describe this sound in 1-2 sentences:\\n{label_str}\"}],\n",
    "        temperature=0.5)\n",
    "    description = desc_response.choices[0].message.content.strip()\n",
    "    \n",
    "    return triples, top_3_labels, description\n",
    "\n",
    "def write_xml(rel_path, triples, top3, summary):\n",
    "    root = Element(\"audio_classification\")\n",
    "    metadata = SubElement(root, \"metadata\")\n",
    "    SubElement(metadata, \"reasoning\").text = f\"{summary} | Top-3: {top3}\"\n",
    "    SubElement(metadata, \"analysis_date\").text = datetime.date.today().isoformat()\n",
    "    for cat in categories:\n",
    "        val, conf, why = triples[cat]\n",
    "        p = SubElement(root, \"parameter\")\n",
    "        p.set(\"name\", str(cat))\n",
    "        p.set(\"value\", f\"{float(val):.4f}\")\n",
    "        p.set(\"confidence\", f\"{float(conf):.4f}\")\n",
    "        p.set(\"reasoning\", why)\n",
    "    xml_path = Path(xml_output_dir) / Path(rel_path).with_suffix(\".xml\")\n",
    "    xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    xml_path.write_bytes(pretty_xml(root))\n",
    "    return str(xml_path)\n",
    "\n",
    "# Process files\n",
    "print(\"=\"*80 + \"\\nPROCESSING AUDIO FILES\\n\" + \"=\"*80 + \"\\n\")\n",
    "csv_rows = []\n",
    "for idx, (full_path, rel_path) in enumerate(audio_files, 1):\n",
    "    print(f\"[{idx}/{len(audio_files)}] {rel_path}\")\n",
    "    try:\n",
    "        triples, top3, desc = analyze_audio(full_path)\n",
    "        row = {\"File\": rel_path, \"Top 3 Labels\": top3, \"Description\": desc}\n",
    "        for cat in categories:\n",
    "            val, conf, why = triples[cat]\n",
    "            row[cat], row[f\"{cat}__conf\"], row[f\"{cat}__reason\"] = val, conf, why\n",
    "        csv_rows.append(row)\n",
    "        xml_path = write_xml(rel_path, triples, top3, desc)\n",
    "        print(f\"  ✅ XML: {xml_path}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ ERROR: {e}\\n\")\n",
    "\n",
    "csv_path_out = os.path.join(folder_path, \"audio_classification_results.csv\")\n",
    "pd.DataFrame(csv_rows).to_csv(csv_path_out, index=False, encoding=\"utf-8\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n✅ CLASSIFICATION COMPLETE!\\n\" + \"=\"*80)\n",
    "print(f\"📄 CSV: {csv_path_out}\")\n",
    "print(f\"📋 XMLs: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80505d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# CELL 4: GENERATE JSON PRESET FILES\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# ▶️ RUN THIS CELL - DO NOT MODIFY!\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nGENERATING JSON PRESET FILES\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "def build_json(name):\n",
    "    return {\"pattrstorage\": {\"name\": name, \"slots\": {}}}\n",
    "\n",
    "input_dir = Path(folder_path)\n",
    "target_dirs = [(\"data-Ablp\", \"blp\"), (\"data-Bblp\", \"blp\"), \n",
    "               (\"data-grain\", \"grain\"), (\"data-stretch\", \"stretch\")]\n",
    "\n",
    "wav_paths = [p for p in input_dir.rglob(\"*\") if p.is_file() and p.suffix.lower() == \".wav\"]\n",
    "print(f\"Found {len(wav_paths)} WAV files\\n\")\n",
    "\n",
    "for target, name in target_dirs:\n",
    "    output_dir = input_dir / target\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for wav in wav_paths:\n",
    "        json_path = output_dir / f\"{wav.stem}.json\"\n",
    "        json_path.write_text(json.dumps(build_json(name), indent=4, ensure_ascii=False))\n",
    "    print(f\"✅ {len(wav_paths)} JSON files → {output_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n✅ JSON PRESET GENERATION COMPLETE!\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee000ebf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✅ SUMMARY & NEXT STEPS\n",
    "\n",
    "## What was created:\n",
    "\n",
    "### 1. Classification XMLs ✅\n",
    "- **Location**: Saved alongside your audio files\n",
    "- **Content**: YAMNet analysis + GPT-4 categorization\n",
    "\n",
    "### 2. CSV Overview ✅\n",
    "- **File**: `audio_classification_results.csv` in your material folder\n",
    "- **Content**: Complete overview of all classifications\n",
    "\n",
    "### 3. JSON Preset Folders ✅\n",
    "Created in your material folder:\n",
    "- `data-Ablp/` - Loop Player A presets\n",
    "- `data-Bblp/` - Loop Player B presets  \n",
    "- `data-grain/` - Granular Synthesizer presets\n",
    "- `data-stretch/` - Time-Stretch Player presets\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Next Steps:\n",
    "\n",
    "### 1. Copy Preset Folders to APO_Main\n",
    "```\n",
    "[your_material_folder]/data-Ablp/    → APO_Main/data-Ablp/\n",
    "[your_material_folder]/data-Bblp/    → APO_Main/data-Bblp/\n",
    "[your_material_folder]/data-grain/   → APO_Main/data-grain/\n",
    "[your_material_folder]/data-stretch/ → APO_Main/data-stretch/\n",
    "```\n",
    "\n",
    "### 2. Backup Material to NAS\n",
    "- Copy your entire material folder to NAS server **data-pg8**\n",
    "\n",
    "### 3. Refine Presets in Max\n",
    "- Open Max patch\n",
    "- Load and test new sounds  \n",
    "- Adjust preset parameters for desired results\n",
    "\n",
    "### 4. Update Partitur Files\n",
    "- Create preset combinations in Max\n",
    "- Save to `partitur.txt` (outdoor) or `partitur_in.txt` (indoor)\n",
    "- Mirror partitur files to NAS **data-pg8** for brain control\n",
    "\n",
    "---\n",
    "\n",
    "**For detailed instructions, see: `readme.md` Section 10.4**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
